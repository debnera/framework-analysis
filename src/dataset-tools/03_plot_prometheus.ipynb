{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T08:51:24.471280Z",
     "start_time": "2024-09-30T08:51:23.835324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = \"../../data/minimized/ov_vs_pytorch/ov-cpu_2mbps-rerun/intermediate/full.feather\"\n",
    "df = pd.read_feather(path)"
   ],
   "id": "c13a9983a7c8f3bc",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T08:55:57.942265Z",
     "start_time": "2024-09-30T08:55:57.879037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "encountered_names = []\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "for column_json_str in df.columns:\n",
    "    if \"timestamp\" == column_json_str:\n",
    "        continue\n",
    "    # print(column_json_str)\n",
    "    column_data = json.loads(column_json_str)  # Parse the JSON string into a Python dictionary\n",
    "    column_name = column_data[\"__name__\"]\n",
    "    if \"bytesin\" in column_name:\n",
    "        if column_name in encountered_names:\n",
    "            pass\n",
    "            print(f\"{column_data}\")\n",
    "        else:\n",
    "            encountered_names.append(column_name)\n",
    "            print(f\"{column_data}\")"
   ],
   "id": "29c85aff33410fe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'__name__': 'kafka_server_brokertopicmetrics_bytesin_total', 'instance': 'kafka_broker_1:11001', 'job': 'kafka'}\n",
      "{'__name__': 'kafka_server_brokertopicmetrics_bytesin_total', 'instance': 'kafka_broker_1:11001', 'job': 'kafka', 'topic': '__consumer_offsets'}\n",
      "{'__name__': 'kafka_server_brokertopicmetrics_bytesin_total', 'instance': 'kafka_broker_1:11001', 'job': 'kafka', 'topic': 'yolo_input'}\n",
      "{'__name__': 'kafka_server_brokertopicmetrics_bytesin_total', 'instance': 'kafka_broker_1:11001', 'job': 'kafka', 'topic': 'yolo_output'}\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T12:30:49.726277Z",
     "start_time": "2024-09-23T12:30:49.661452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "encountered_names = []\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "for column_json_str in df.columns:\n",
    "    if \"timestamp\" == column_json_str:\n",
    "        continue\n",
    "    # print(column_json_str)\n",
    "    column_data = json.loads(column_json_str)  # Parse the JSON string into a Python dictionary\n",
    "    column_name = column_data[\"__name__\"]\n",
    "    if \"container\" in column_name:\n",
    "        if column_name in encountered_names:\n",
    "            pass\n",
    "            # print(f\"The name '{column}' contains 'cgroup' and has been encountered before.\")\n",
    "        else:\n",
    "            encountered_names.append(column_name)\n",
    "            print(f\"The name '{column_name}' contains 'cgroup' and has not been encountered before.\")"
   ],
   "id": "fe1ee6e20fddd079",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name 'container_memory_failures_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_blkio_device_usage_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_fs_reads_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_fs_writes_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_cpu_usage_seconds_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_cpu_user_seconds_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_cpu_system_seconds_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_memory_usage_bytes' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_memory_working_set_bytes' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_memory_rss' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_memory_cache' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_memory_mapped_file' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_memory_swap' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_processes' contains 'cgroup' and has not been encountered before.\n",
      "The name 'kepler_container_package_joules_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'kepler_container_core_joules_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'kepler_container_uncore_joules_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'kepler_container_joules_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_fs_reads_bytes_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_fs_writes_bytes_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'kepler_container_cpu_instructions_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'kepler_container_cpu_cycles_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'kepler_container_cache_miss_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'kepler_container_task_clock_ms_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'kepler_container_bpf_cpu_time_ms_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'kepler_container_bpf_net_rx_irq_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_fs_inodes_free' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_fs_usage_bytes' contains 'cgroup' and has not been encountered before.\n",
      "The name 'kepler_container_bpf_block_irq_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_last_seen' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_network_transmit_bytes_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_network_receive_bytes_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_network_transmit_packets_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_network_receive_packets_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_threads' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_network_receive_packets_dropped_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_cpu_cfs_throttled_seconds_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_cpu_cfs_periods_total' contains 'cgroup' and has not been encountered before.\n",
      "The name 'container_cpu_cfs_throttled_periods_total' contains 'cgroup' and has not been encountered before.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "bc2e9110cdce6c34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T08:31:22.975907Z",
     "start_time": "2024-09-23T08:31:22.734874Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Load dataframe and remove columns with static or missing values\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "path = \"../../data/processed/partA/prom/prom_ov/worker1.feather\"\n",
    "df = pd.read_feather(path)\n",
    "\n",
    "# Count number of rows and cols in the original df\n",
    "print(f\"Loaded {len(df)} rows and {len(df.columns)} columns\")\n",
    "\n",
    "# Count the number of unique values in each column\n",
    "unique_counts = df.nunique()\n",
    "\n",
    "# Find all static columns (columns with only one or two unique values)\n",
    "static_columns = unique_counts[unique_counts <= 2].index\n",
    "\n",
    "# Remove the static columns from the dataframe\n",
    "df = df.drop(static_columns, axis=1)\n",
    "print(f\"Removing {len(static_columns)} static columns ({len(df.columns)} remaining)\")\n",
    "\n",
    "if len(df.columns) < 100:\n",
    "    # Only display if the df is small enough to not stall the IDE (thousands of columns really slows things down)\n",
    "    df.head()"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/processed/partA/prom/prom_ov/worker1.feather'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      6\u001B[0m path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../../data/processed/partA/prom/prom_ov/worker1.feather\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 7\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_feather\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Count number of rows and cols in the original df\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLoaded \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(df)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m rows and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(df\u001B[38;5;241m.\u001B[39mcolumns)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m columns\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/framework-analysis/lib/python3.8/site-packages/pandas/io/feather_format.py:144\u001B[0m, in \u001B[0;36mread_feather\u001B[0;34m(path, columns, use_threads, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m feather\n\u001B[1;32m    142\u001B[0m check_dtype_backend(dtype_backend)\n\u001B[0;32m--> 144\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[1;32m    146\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m handles:\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m dtype_backend \u001B[38;5;129;01mis\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mno_default:\n\u001B[1;32m    148\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m feather\u001B[38;5;241m.\u001B[39mread_feather(\n\u001B[1;32m    149\u001B[0m             handles\u001B[38;5;241m.\u001B[39mhandle, columns\u001B[38;5;241m=\u001B[39mcolumns, use_threads\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mbool\u001B[39m(use_threads)\n\u001B[1;32m    150\u001B[0m         )\n",
      "File \u001B[0;32m~/miniconda3/envs/framework-analysis/lib/python3.8/site-packages/pandas/io/common.py:868\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    859\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    860\u001B[0m             handle,\n\u001B[1;32m    861\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    864\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    865\u001B[0m         )\n\u001B[1;32m    866\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    867\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m--> 868\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    869\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[1;32m    871\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../../data/processed/partA/prom/prom_ov/worker1.feather'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "366da6d4890f478b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92642772ff7e0e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T08:31:23.054189703Z",
     "start_time": "2024-08-23T05:53:37.785455Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Change the dataframe headers to a more human-readable format\n",
    "\"\"\"\n",
    "\n",
    "from utils.header_cleaner import clean_up_headers\n",
    "cleaned_df = clean_up_headers(df)\n",
    "cleaned_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ca89f0479eaa1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T08:31:23.054756710Z",
     "start_time": "2024-08-23T05:53:41.478902Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Extend this script to do some plotting (maybe copy-and-paste this code into separate file first)\n",
    "\"\"\"\n",
    "\n",
    "import difflib\n",
    "target_word = 'kepler node joules total'\n",
    "closest_matches = difflib.get_close_matches(target_word, cleaned_df.columns, n = 6,  cutoff=0.05)\n",
    "print(closest_matches)\n",
    "cleaned_df[closest_matches].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
