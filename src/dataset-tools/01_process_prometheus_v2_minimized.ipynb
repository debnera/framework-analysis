{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os.path\n",
    "import time\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "import os\n",
    "from utils import utils\n",
    "\n",
    "\"\"\"\n",
    "00: Configuration and imports\n",
    "\n",
    "NOTE: If using DataSpell, or similar IDE, you may need to increase the maximum allowed memory usage.\n",
    "- Larger datasets will cause the IDE to completely freeze with the default limit of 4GB ram.\n",
    "\n",
    "Results are minimized by removing all columns with static values.\n",
    "- This means that some dataframes might have different columns than other dataframes.\n",
    "\"\"\"\n",
    "\n",
    "# This script will process all zips located at the input_path\n",
    "# input_path = \"../../data/raw_datasets/8.8_ajot\"\n",
    "# input_path = \"/home/anton/Downloads/ov-ajo\"\n",
    "# input_path = \"../../data/raw_datasets/ov_vs_pytorch\"\n",
    "# output_path = \"../../data/processed/ov_vs_pytorch/prom\"\n",
    "input_path = \"../../data/raw_datasets/\"\n",
    "output_path = \"../../data/minimized/\"\n",
    "\n",
    "zip_files_list = utils.list_zip_files(input_path)\n",
    "\n",
    "print(\"List of zip files:\")\n",
    "for zip_file in zip_files_list:\n",
    "    print(zip_file)"
   ],
   "id": "967834d7704352bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "01: Helper functions\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm  # Import the tqdm module for progress bar\n",
    "import utils.prometheus_processing as prom_util\n",
    "\n",
    "import py7zr\n",
    "\n",
    "# Open the .7z file\n",
    "\n",
    "def get_slices(zip_file, size_limit_mb):\n",
    "    if zip_file.endswith(\".zip\"):\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            items = zip_ref.namelist()\n",
    "            json_files = [x for x in items if x.endswith('.json')]\n",
    "            json_files_info = [(x, zip_ref.getinfo(x)) for x in json_files]\n",
    "            json_files_info = sorted(json_files_info, key=lambda x: x[1].file_size, reverse=True)\n",
    "    else:\n",
    "        print(f\"Cannot parse {zip_file}\")\n",
    "\n",
    "    total_file_size = sum(info.file_size for _, info in json_files_info)\n",
    "    slice_limit = size_limit_mb * 1024 * 1024  # 100MB in bytes\n",
    "    slices = []\n",
    "    current_slice = []\n",
    "    current_size = 0\n",
    "\n",
    "    for file_name, file_info in json_files_info:\n",
    "        if current_size + file_info.file_size <= slice_limit:\n",
    "            current_slice.append(file_name)\n",
    "            current_size += file_info.file_size\n",
    "        else:\n",
    "            slices.append(current_slice)\n",
    "            current_slice = [file_name]\n",
    "            current_size = file_info.file_size\n",
    "\n",
    "    if current_slice:\n",
    "        slices.append(current_slice)\n",
    "\n",
    "    return slices\n",
    "\n",
    "def parse_slice(zip_file, slice):\n",
    "    values_container = {}\n",
    "    index = 0\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        for path in slice:\n",
    "            size_in_megabytes = zip_ref.getinfo(path).file_size / (1024 * 1024)\n",
    "            print(f\"\\t{index}: {size_in_megabytes} MB, {path}\")\n",
    "            index += 1\n",
    "            with zip_ref.open(path) as json_file:\n",
    "                parse_metric(json_file, path, values_container)\n",
    "\n",
    "    values_df = pd.DataFrame(values_container).apply(pd.to_numeric,\n",
    "                                                     errors='ignore')  # Move to numeric if possible, cutting off 90% of size\n",
    "    return values_df\n",
    "\n",
    "\n",
    "def parse_metric(data, path, values_container):\n",
    "        json_data = json.load(data)\n",
    "        # print(path)\n",
    "\n",
    "        # LOOP THROUGH EACH SUB-METRIC\n",
    "        try:\n",
    "            for item in json_data['data']['result']:\n",
    "                header = json.dumps(item['metric']) # Use a tuple of the metric dictionary's items\n",
    "                values = dict(item['values'])\n",
    "\n",
    "                # ADD HEADER KEY TO VALUES DICT\n",
    "                if header not in values_container:\n",
    "                    values_container[header] = {}\n",
    "                values_container[header].update(values)\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"KeyError occurred while parsing JSON file '{path}': {e}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"ValueError occurred while parsing JSON file '{path}': {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while parsing JSON file '{path}': {e}\")\n",
    "\n",
    "\n",
    "def main(input_path, zip_relative_path, output_path2):\n",
    "    dfs = []\n",
    "    print(f\"Processing {zip_relative_path}\")\n",
    "    zip_name = zip_relative_path.replace(\".zip\", \"\")  # Remove file-extension for now\n",
    "    full_output_path = f\"{output_path2}/{zip_name}\"\n",
    "    intermediate_folder_path = f\"{full_output_path}/intermediate\"\n",
    "    processed_folder_path = f\"{full_output_path}/\"\n",
    "    start_time = time.time()\n",
    "    max_slice_size_mb = 200\n",
    "    slices = get_slices(f\"{input_path}/{zip_relative_path}\", max_slice_size_mb)\n",
    "    for i, slice in enumerate(slices):\n",
    "        os.makedirs(intermediate_folder_path, exist_ok=True)\n",
    "        output_path = intermediate_folder_path + f\"/{i}.feather\"\n",
    "        if os.path.exists(output_path):\n",
    "            values = pd.read_feather(output_path)\n",
    "            print(f\"Got intermediate file from {output_path}\")\n",
    "        else:\n",
    "            values = parse_slice(\n",
    "                zip_file=f'{input_path}/{zip_relative_path}',\n",
    "                slice=slice,\n",
    "            )\n",
    "            # values = values.apply(pd.to_numeric, errors='coerce')\n",
    "            values.reset_index(drop=False, inplace=True, names=[\"timestamp\"])  # Reset to default index (in case of old pandas/pyarrow version)\n",
    "            unique_counts = values.nunique()\n",
    "            static_columns = unique_counts[unique_counts <= 2].index\n",
    "            values.drop(static_columns, axis=1, inplace=True)\n",
    "            if len(values) == 0:\n",
    "                # Cannot save empty dataframes - nothing to do here\n",
    "                continue\n",
    "            try:\n",
    "                values.to_feather(output_path)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            print(f\"Saved intermediate {output_path}\")\n",
    "        values.index = values[\"timestamp\"]\n",
    "        values.drop(columns=[\"timestamp\"], inplace=True)\n",
    "        dfs.append(values)\n",
    "    \n",
    "    try:\n",
    "        df = pd.concat(dfs, axis=1)\n",
    "    except Exception as e:\n",
    "        # This can happen if the zip did not contain any prometheus data (e.g., it contains yolo-data only)\n",
    "        print(e)\n",
    "        return\n",
    "    df = df.loc[:,\n",
    "         ~df.columns.duplicated()]  # TODO: Does removing duplicates remove information? Happens probably at zip-file slice boundaries\n",
    "    df = df.reset_index(drop=False, inplace=False, names=[\"timestamp\"])  # Reset to default index (in case of old pandas/pyarrow version)\n",
    "    df.to_feather(intermediate_folder_path + f\"/full.feather\")\n",
    "    df.index = df[\"timestamp\"]\n",
    "    df.drop(columns=[\"timestamp\"], inplace=True)\n",
    "    \n",
    "    print(f\"Saved full df to {intermediate_folder_path}/full.feather\")\n",
    "\n",
    "    # df.index = df[\"timestamp\"]  # Re-add index (in case of old pandas/pyarrow version)\n",
    "\n",
    "    # Split df by instance\n",
    "    sub_dfs = prom_util.sub_df_by_instance(df)\n",
    "\n",
    "    # Minimize headers and save each instance as separate file\n",
    "    for instance, sub_df in sub_dfs.items():\n",
    "        df_minimized = sub_df.copy()\n",
    "        # df_minimized.index = df_minimized[\"timestamp\"] # \n",
    "        # df_minimized.drop(\"index\", axis=1, inplace=True)\n",
    "\n",
    "        # Group headers by name\n",
    "        grouped_by_name = {}\n",
    "        for col in list(df_minimized.columns):\n",
    "            header_dict = json.loads(col)\n",
    "            name = header_dict[\"__name__\"]\n",
    "            if name not in grouped_by_name:\n",
    "                grouped_by_name[name] = {}\n",
    "            grouped_by_name[name][col] = header_dict\n",
    "\n",
    "        # Minimize headers\n",
    "        for feature_name, headers in grouped_by_name.items():\n",
    "            non_match_count = 0\n",
    "            try:\n",
    "                descriptive_keys = prom_util.get_descriptive_keys(headers)\n",
    "            except:\n",
    "                # print(f\"Non-matching keys: {feature_name}\")\n",
    "                non_match_count += 1\n",
    "                continue\n",
    "            if non_match_count > 0:\n",
    "                # TODO: Does this mean that information is removed from the resulting dataframe or just a debug print?\n",
    "                print(f\"Non-matching keys: {non_match_count}\")\n",
    "            prom_util.remove_unnecessary_keys(df_minimized, headers, descriptive_keys)\n",
    "\n",
    "        # Save df\n",
    "        path = f\"{processed_folder_path}\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        # df_minimized = df_minimized.sort_index()  # Make sure the dataframe is sorted by timestamp\n",
    "        # df_minimized.index = df_minimized[\"index\"]\n",
    "        df_minimized = df_minimized.sort_index().reset_index(drop=False, inplace=False, names=[\"timestamp\"])\n",
    "        # print(df_minimized.index)\n",
    "        df_minimized.to_feather(path + f\"/{instance}.feather\")\n",
    "        print(\"Saved instanced df as\", path + f\"/{instance}.feather\")\n",
    "\n",
    "\"\"\"\n",
    "02: Process and save dataframes\n",
    "\"\"\"\n",
    "\n",
    "zips = utils.list_zip_files(input_path)\n",
    "print(zips)\n",
    "\n",
    "for zip_name_full in zips:\n",
    "    print(zip_name_full)\n",
    "    main(input_path, zip_name_full, output_path)\n"
   ],
   "id": "5188e0c5023d7d91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "03: Print some statistics from the resulting dataframes\n",
    "\n",
    "- Mostly for quick sanity checking of the results\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "def count_feather_files(fpath):\n",
    "    feather_files = []\n",
    "    file_info = []\n",
    "    for root, dirs, files in os.walk(fpath):\n",
    "        for file in files:\n",
    "            if file.endswith(\".feather\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                feather_files.append(file_path)\n",
    "\n",
    "    for file_path in feather_files:\n",
    "        try:\n",
    "            df = pd.read_feather(file_path)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            file_info.append((file_path, file_size, len(df.columns), len(df)))\n",
    "        except pd.errors.EmptyDataError:\n",
    "            file_info.append((file_path, 0, 0, 0))\n",
    "        except Exception as e:\n",
    "            file_info.append((file_path, -1, -1, -1))\n",
    "\n",
    "    file_info.sort(key=lambda x: x[1], reverse=True)  # Sort based on file size in descending order\n",
    "\n",
    "    for info in file_info:\n",
    "        print(\"Size:\", info[1] / 10**6, \"mb\", end=\"\\t\")\n",
    "        print(\"Cols:\", info[2], end=\"\\t\")\n",
    "        print(\"Rows:\", info[3], end=\"\\t\")\n",
    "        print(\"File:\", info[0].replace(fpath, \"\"))\n",
    "\n",
    "# Provide the path to the folder containing the feather files\n",
    "count_feather_files(output_path)"
   ],
   "id": "f9a6a87f6243517f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2c07873428d25d2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
