{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "from pandas.core.interchange.dataframe_protocol import DataFrame\n",
    "\n",
    "\n",
    "# Function to find the subfolders with the file names\n",
    "def find_subfolders_with_file(root_folder, filename):\n",
    "    result = []\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        if filename in files:\n",
    "            result.append(root)\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "root_folder = '../../../data/minimized'\n",
    "filename = 'worker1.feather'\n",
    "subfolders = find_subfolders_with_file(root_folder, filename)\n",
    "# subfolders = [x for x in subfolders if \"yolov9\" in x or \"yolov10\" in x]\n",
    "print(subfolders)"
   ],
   "id": "6a5183090ba2659d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def read_feather_cached(file_path):\n",
    "    return pd.read_feather(file_path)"
   ],
   "id": "ad90a1f2475eed8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "from utils.header_cleaner import *\n",
    "import difflib\n",
    "import os\n",
    "\n",
    "# It's really quite simple - we're comparing PyTorch and OpenVINO YOLOv8n performance\n",
    "# Some easy things to compare would be power, and latency\n",
    "\n",
    "def clean_and_calculate_power(dataframe):\n",
    "    cleaned_df = clean_df(dataframe)\n",
    "    cleaned_df.sort_values(by=\"timestamp\", inplace=True)\n",
    "    # Target word matching and plotting\n",
    "    target_word = 'kepler node joules total dynamic'\n",
    "    closest_matches = difflib.get_close_matches(target_word, cleaned_df.columns, n=6, cutoff=0.05)\n",
    "    print(closest_matches)\n",
    "    cleaned_df['total_joules'] = cleaned_df[closest_matches].sum(axis=1)\n",
    "    ts = cleaned_df[\"timestamp\"]\n",
    "    interval = ts[1] - ts[0]\n",
    "    cleaned_df['power_consumed'] = cleaned_df['total_joules'].diff() / interval\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "prom_data_paths = {'ov_cpu_path': \"../../../data/processed/ov_vs_pytorch/prom/ov-cpu_2mbps-rerun/\",\n",
    "                   'pytorch_path': \"../../../data/processed/ov_vs_pytorch/prom/pytorch_2mbps/\"}\n",
    "prom_data_paths = {os.path.basename(x): x for x in subfolders}\n",
    "yolo_data_paths = {key: os.path.join(val, \"yolo_qos.feather\") for key, val in prom_data_paths.items()}\n",
    "\n",
    "total_power = {}\n",
    "response_time = {}\n",
    "for key in prom_data_paths.keys():\n",
    "    paths = []\n",
    "    for work_num in range(1, 6):\n",
    "        temp_path = os.path.join(prom_data_paths[key], f\"worker{work_num}.feather\")\n",
    "        print(temp_path)\n",
    "        paths.append(temp_path)\n",
    "    concatenated_power = pd.concat([df['power_consumed'] for df in [clean_and_calculate_power(read_feather_cached(x)) for x in paths]], axis=1)\n",
    "    total_power[key] = concatenated_power.mean(axis=1)\n",
    "\n",
    "for key in prom_data_paths.keys():\n",
    "    yolo_df = read_feather_cached(yolo_data_paths[key])\n",
    "    yolo_df['total_inference_time'] = yolo_df['inf'] + yolo_df['post'] + yolo_df['pre']\n",
    "    yolo_df['end_to_end_response_time'] = yolo_df['total_inference_time'] + yolo_df['queue']\n",
    "    yolo_df['start'] = pd.to_datetime(yolo_df['start_time'], unit='ms')  # Convert to datetime (optional)\n",
    "    yolo_df.set_index('start', inplace=True)\n",
    "    resampled_df = yolo_df.resample('5S')\n",
    "    response_time[key] = resampled_df.agg({'end_to_end_response_time': 'mean'}).reset_index()['end_to_end_response_time'].rename(key)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_rate(dataframe, target_word):\n",
    "    cleaned_df = dataframe.copy()\n",
    "    cleaned_df.sort_values(by=\"timestamp\", inplace=True)\n",
    "    closest_matches = difflib.get_close_matches(target_word, cleaned_df.columns, n=6, cutoff=0.05)\n",
    "    match = closest_matches[0]\n",
    "    print(match)\n",
    "    cleaned_df['total_count'] = cleaned_df[match]\n",
    "    ts = cleaned_df[\"timestamp\"]\n",
    "    interval = ts[1] - ts[0]\n",
    "    cleaned_df['rate'] = cleaned_df['total_count'].diff() / interval\n",
    "    return cleaned_df['rate']\n",
    "\n",
    "if 'rate_df_cache' not in globals():\n",
    "    rate_df_cache = {}\n",
    "\n",
    "def precompute_rates(keyword):\n",
    "    for key in prom_data_paths.keys():\n",
    "        full_key = key + keyword\n",
    "        if full_key in rate_df_cache:\n",
    "            continue\n",
    "        path = prom_data_paths[key] + \"/\" + \"intermediate/full.feather\"\n",
    "        df = read_feather_cached(path)\n",
    "        rate_df_cache[full_key] = calculate_rate(df, keyword)\n",
    "\n",
    "precompute_rates(\"kafka_server_brokertopicmetrics_bytesin_total yolo_output\")"
   ],
   "id": "222913acdb301108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_rate_by_model(keyword, title):\n",
    "    grouped_df_dict = {}\n",
    "    for key in prom_data_paths.keys():\n",
    "        path = prom_data_paths[key] + \"/\" + \"intermediate/full.feather\"\n",
    "        df = read_feather_cached(path)\n",
    "        _, model_name, resolution = key.split('_')\n",
    "        if model_name not in grouped_df_dict:\n",
    "            grouped_df_dict[model_name] = {}\n",
    "        grouped_df_dict[model_name][resolution] = key+keyword\n",
    "    \n",
    "    # Sort model names\n",
    "    sorted_model_names = sorted(grouped_df_dict.keys())\n",
    "    \n",
    "    # Use the cached rates to plot for each model name\n",
    "    for model_name in sorted_model_names:\n",
    "        res_dict = grouped_df_dict[model_name]\n",
    "        sorted_resolutions = sorted(res_dict.keys())\n",
    "        rate_df_dict = {}\n",
    "        for resolution in sorted_resolutions:\n",
    "            key = res_dict[resolution]\n",
    "            rate_df_dict[resolution] = rate_df_cache[key]\n",
    "        \n",
    "        rate_df = pd.DataFrame.from_dict(rate_df_dict)\n",
    "        fig = px.line(rate_df, x=rate_df.index, y=rate_df.columns, title=f\"{title} - {model_name}\")\n",
    "        fig.update_layout(xaxis_title='Time')\n",
    "        fig.show()\n",
    "\n",
    "def plot_resolution_by_model(keyword, title):\n",
    "    resolution_df_dict = {}\n",
    "    for key in prom_data_paths.keys():\n",
    "        path = prom_data_paths[key] + \"/\" + \"intermediate/full.feather\"\n",
    "        df = read_feather_cached(path)\n",
    "        _, model_name, resolution = key.split('_')\n",
    "        if resolution not in resolution_df_dict:\n",
    "            resolution_df_dict[resolution] = {}\n",
    "        resolution_df_dict[resolution][model_name] = key+keyword\n",
    "    \n",
    "    # Sort resolutions\n",
    "    sorted_resolutions = sorted(resolution_df_dict.keys())\n",
    "    \n",
    "    # Use the cached rates to plot for each resolution\n",
    "    for resolution in sorted_resolutions:\n",
    "        model_dict = resolution_df_dict[resolution]\n",
    "        sorted_model_names = sorted(model_dict.keys())\n",
    "        rate_df_dict = {}\n",
    "        for model_name in sorted_model_names:\n",
    "            key = model_dict[model_name]\n",
    "            rate_df_dict[model_name] = rate_df_cache[key]\n",
    "        \n",
    "        rate_df = pd.DataFrame.from_dict(rate_df_dict)\n",
    "        fig = px.line(rate_df, x=rate_df.index, y=rate_df.columns, title=f\"{title} - resolution: {resolution}\")\n",
    "        fig.update_layout(xaxis_title='Time')\n",
    "        fig.show()"
   ],
   "id": "e657c813fef02411",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_resolution_by_model(\"kafka_server_brokertopicmetrics_bytesin_total yolo_output\", \"yolo output rate\")",
   "id": "e331dc353a8be38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "717d91d34e823ef2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b8c214b54b682bfe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
