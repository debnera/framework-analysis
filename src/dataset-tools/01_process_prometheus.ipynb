{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967834d7704352bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T06:22:18.466789Z",
     "start_time": "2024-08-20T06:22:18.462415Z"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "00: Configuration and imports\n",
    "\n",
    "NOTE: If using DataSpell, or similar IDE, you may need to increase the maximum allowed memory usage.\n",
    "- Larger datasets will cause the IDE to completely freeze with the default limit of 4GB ram.\n",
    "\"\"\"\n",
    "\n",
    "# This script will process all zips located at the input_path\n",
    "input_path = \"../../data/raw_datasets\"\n",
    "output_path = \"../../data/processed/19.8_ajo/prom\"\n",
    "\n",
    "\n",
    "# Print all zips that will be processed\n",
    "def list_zip_files(directory_path):\n",
    "    zip_files = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".zip\"):\n",
    "                # zip_files.append(file)\n",
    "                full_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(full_path, directory_path)\n",
    "                zip_files.append(relative_path)\n",
    "    return zip_files\n",
    "zip_files_list = list_zip_files(input_path)\n",
    "\n",
    "print(\"List of zip files:\")\n",
    "for zip_file in zip_files_list:\n",
    "    print(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5188e0c5023d7d91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T06:23:34.442035Z",
     "start_time": "2024-08-20T06:22:21.719003Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "01: Helper functions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def parse_metric(zip_file, slice=-1, total_slices=-1, start_time=time.time()):\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "\n",
    "        # GENERATE A LIST OF ALL CONTAINED FILES\n",
    "        # AND CONSTRUCT THE FILE PREFIX FOR THE METRIC FILES\n",
    "        items = zip_ref.namelist()\n",
    "\n",
    "        # FILTER ONLY THE JSON FILES\n",
    "        json_files = [x for x in items if x.endswith('.json')]\n",
    "        json_files.sort()\n",
    "\n",
    "        # TEMP CONTAINERS\n",
    "        values_container = {}\n",
    "        stats_container = {}\n",
    "\n",
    "        number_of_files = len(json_files)\n",
    "        # start_time = time.time()\n",
    "        # LOOP THROUGH THE JSON FILES\n",
    "        if slice >= 0 and slice < number_of_files:\n",
    "            files_per_slice = number_of_files // total_slices\n",
    "            start = slice * files_per_slice\n",
    "            end = min(start + files_per_slice, number_of_files)\n",
    "        else:\n",
    "            start = 0\n",
    "            end = number_of_files\n",
    "        # debug_start = int(0.99 * number_of_files)\n",
    "        count = start\n",
    "        print(start)\n",
    "        print(end)\n",
    "        for path in json_files[start:end]:\n",
    "            count += 1\n",
    "            with zip_ref.open(path) as json_file:\n",
    "                json_data = json.load(json_file)\n",
    "                print(path)\n",
    "\n",
    "                # LOOP THROUGH EACH SUB-METRIC\n",
    "                try:\n",
    "                    for item in json_data['data']['result']:\n",
    "                        header = json.dumps(item['metric'])\n",
    "                        values = dict(item['values'])\n",
    "    \n",
    "                        # ADD HEADER KEY IF IT DOESNT EXIST\n",
    "                        if header not in values_container:\n",
    "                            values_container[header] = {}\n",
    "    \n",
    "                        # OTHERWISE, MERGE OLD AND NEW DICTS\n",
    "                        values_container[header].update(values)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(f\"Could not parse {json_file}\")\n",
    "                    \n",
    "\n",
    "            # if True:\n",
    "            if count % 500 == 0 or count == end:\n",
    "                print(\n",
    "                    f\"Progress {count}/{number_of_files}, ({count / number_of_files * 100} %) (time_spent: {time.time() - start_time} s  - avg: {(time.time() - start_time) / count} s)\")\n",
    "\n",
    "        print(\"Done - converting to dataframe\")\n",
    "        # CONVERT VALUES DICT TO DATAFRAME\n",
    "        values_df = pd.DataFrame(values_container).apply(pd.to_numeric,\n",
    "                                                         errors='ignore')  # Move to numeric if possible, cutting off 90% of size\n",
    "\n",
    "        # CONVERT THE STATS DICT TO A DF, THEN TRANSPOSE IT\n",
    "        print(\"Done - transposing dataframe\")\n",
    "        stats_df = pd.DataFrame(stats_container).transpose()\n",
    "\n",
    "        print(\"Done\")\n",
    "        return values_df, stats_df\n",
    "\n",
    "\n",
    "def get_descriptive_keys(header_group):\n",
    "    headers = header_group\n",
    "    first_elem = list(headers.values())[0]\n",
    "    first_keys = list(first_elem.keys())\n",
    "    # first_keys = list(headers.keys())[0].keys()\n",
    "    blacklist = []\n",
    "\n",
    "    for key, val in headers.items():\n",
    "        assert first_keys == list(val.keys()), 'ALL HEADER KEYS DO NOT MATCH'\n",
    "\n",
    "    for key in first_keys:\n",
    "        # is_static = always_matches(headers, lambda x: headers[0][key] == x[key])\n",
    "        is_static = True\n",
    "        for _, val in headers.items():\n",
    "            if not first_elem[key] == val[key]:\n",
    "                is_static = False\n",
    "        # print(is_static)\n",
    "        if is_static:\n",
    "            blacklist.append(key)\n",
    "    print(blacklist)\n",
    "    # print(set(first_keys) ^ set(blacklist))\n",
    "    descriptive_keys = set(first_keys) - (\n",
    "                set(blacklist) ^ {\"__name__\"} ^ {\"instance\"})  # Keep descriptive keys and the name\n",
    "    return descriptive_keys\n",
    "\n",
    "\n",
    "def remove_unnecessary_keys(df, cols, descriptive_keys):\n",
    "    # LOAD ORIGINAL COLUMN AS DICT, THEN NUKE THE REPETITIVE KEYS\n",
    "    # FINALLY, CONVERT PRODUCT BACK TO STRING\n",
    "    new_cols = []\n",
    "    original_cols = cols\n",
    "    for col in original_cols:\n",
    "        as_dict = json.loads(col)\n",
    "        repetitive_headers = set(as_dict.keys()) ^ descriptive_keys\n",
    "\n",
    "        for bad_header in repetitive_headers:\n",
    "            del as_dict[bad_header]\n",
    "\n",
    "        as_string = json.dumps(as_dict)\n",
    "        new_cols.append(as_string)\n",
    "\n",
    "    # MAKE DICT OF RENAMED COLUMNS, AND MODIFY THE DF\n",
    "    cols_swaps = dict(zip(original_cols, new_cols))\n",
    "    df.rename(columns=cols_swaps, inplace=True)\n",
    "\n",
    "\n",
    "def sub_df_by_instance(df):\n",
    "    sub_df_cols = {}\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            col_as_dict = json.loads(col)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"Could not parse {col}\")\n",
    "            continue\n",
    "        if \"instance\" not in col_as_dict:\n",
    "            instance = \"unknown\"\n",
    "        else:\n",
    "            instance = col_as_dict[\"instance\"]\n",
    "        if instance not in sub_df_cols:\n",
    "            sub_df_cols[instance] = []\n",
    "        sub_df_cols[instance].append(col)\n",
    "    sub_dfs = {instance: df[cols] for instance, cols in sub_df_cols.items()}\n",
    "    return sub_dfs\n",
    "\n",
    "\n",
    "def main(input_path, zip_relative_path, output_path2):\n",
    "    dfs = []\n",
    "    print(f\"Processing {zip_relative_path}\")\n",
    "    zip_name = zip_relative_path.replace(\".zip\", \"\")  # Remove file-extension for now\n",
    "    full_output_path = f\"{output_path2}/{zip_name}\"\n",
    "    intermediate_folder_path = f\"{full_output_path}/intermediate\"\n",
    "    processed_folder_path = f\"{full_output_path}/\"\n",
    "    start_time = time.time()\n",
    "    num_slices = 100  # In how many slices should the data be processed - lower values consume more memory\n",
    "    for slice in range(num_slices):\n",
    "        os.makedirs(intermediate_folder_path, exist_ok=True)\n",
    "        output_path = intermediate_folder_path + f\"/{slice}.feather\"\n",
    "        if os.path.exists(output_path):\n",
    "            values = pd.read_feather(output_path)\n",
    "            print(f\"Got intermediate file from {output_path}\")\n",
    "        else:\n",
    "            values, stats = parse_metric(\n",
    "                zip_file=f'{input_path}/{zip_relative_path}',\n",
    "                slice=slice,\n",
    "                total_slices=num_slices,\n",
    "                start_time=start_time\n",
    "            )\n",
    "            # values = values.apply(pd.to_numeric, errors='coerce')\n",
    "            values.reset_index(drop=False, inplace=True, names=[\"timestamp\"])  # Reset to default index (in case of old pandas/pyarrow version)\n",
    "            values.to_feather(output_path)\n",
    "            print(f\"Saved intermediate {output_path}\")\n",
    "        values.index = values[\"timestamp\"]\n",
    "        values.drop(columns=[\"timestamp\"], inplace=True)\n",
    "        dfs.append(values)\n",
    "    \n",
    "    df = pd.concat(dfs, axis=1)\n",
    "    df = df.loc[:,\n",
    "         ~df.columns.duplicated()]  # TODO: Does removing duplicates remove information? Happens probably at zip-file slice boundaries\n",
    "    df = df.reset_index(drop=False, inplace=False, names=[\"timestamp\"])  # Reset to default index (in case of old pandas/pyarrow version)\n",
    "    df.to_feather(intermediate_folder_path + f\"/full.feather\")\n",
    "    df.index = df[\"timestamp\"]\n",
    "    df.drop(columns=[\"timestamp\"], inplace=True)\n",
    "    \n",
    "    print(f\"Saved full df to {intermediate_folder_path}/full.feather\")\n",
    "\n",
    "    # df.index = df[\"timestamp\"]  # Re-add index (in case of old pandas/pyarrow version)\n",
    "\n",
    "    # Split df by instance\n",
    "    sub_dfs = sub_df_by_instance(df)\n",
    "\n",
    "    # Minimize headers and save each instance as separate file\n",
    "    for instance, sub_df in sub_dfs.items():\n",
    "        df_minimized = sub_df.copy()\n",
    "        # df_minimized.index = df_minimized[\"timestamp\"] # \n",
    "        # df_minimized.drop(\"index\", axis=1, inplace=True)\n",
    "\n",
    "        # Group headers by name\n",
    "        grouped_by_name = {}\n",
    "        for col in list(df_minimized.columns):\n",
    "            header_dict = json.loads(col)\n",
    "            name = header_dict[\"__name__\"]\n",
    "            if name not in grouped_by_name:\n",
    "                grouped_by_name[name] = {}\n",
    "            grouped_by_name[name][col] = header_dict\n",
    "\n",
    "        # Minimize headers\n",
    "        for feature_name, headers in grouped_by_name.items():\n",
    "            try:\n",
    "                descriptive_keys = get_descriptive_keys(headers)\n",
    "            except:\n",
    "                print(f\"Non-matching keys: {feature_name}\")\n",
    "                continue\n",
    "            remove_unnecessary_keys(df_minimized, headers, descriptive_keys)\n",
    "\n",
    "        # Save df\n",
    "        path = f\"{processed_folder_path}\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        # df_minimized = df_minimized.sort_index()  # Make sure the dataframe is sorted by timestamp\n",
    "        # df_minimized.index = df_minimized[\"index\"]\n",
    "        df_minimized = df_minimized.sort_index().reset_index(drop=False, inplace=False, names=[\"timestamp\"])\n",
    "        # print(df_minimized.index)\n",
    "        df_minimized.to_feather(path + f\"/{instance}.feather\")\n",
    "        print(\"Saved instanced df as\", path + f\"/{instance}.feather\")\n",
    "\n",
    "\"\"\"\n",
    "02: Process and save dataframes\n",
    "\"\"\"\n",
    "\n",
    "zips = list_zip_files(input_path)\n",
    "print(zips)\n",
    "\n",
    "for zip_name_full in zips:\n",
    "    print(zip_name_full)\n",
    "    main(input_path, zip_name_full, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a6a87f6243517f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T06:23:58.704918Z",
     "start_time": "2024-08-20T06:23:42.725645Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "03: Print some statistics from the resulting dataframes\n",
    "\n",
    "- Mostly for quick sanity checking of the results\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "def count_feather_files(fpath):\n",
    "    feather_files = []\n",
    "    file_info = []\n",
    "    for root, dirs, files in os.walk(fpath):\n",
    "        for file in files:\n",
    "            if file.endswith(\".feather\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                feather_files.append(file_path)\n",
    "\n",
    "    for file_path in feather_files:\n",
    "        try:\n",
    "            df = pd.read_feather(file_path)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            file_info.append((file_path, file_size, len(df.columns), len(df)))\n",
    "        except pd.errors.EmptyDataError:\n",
    "            file_info.append((file_path, 0, 0, 0))\n",
    "        except Exception as e:\n",
    "            file_info.append((file_path, -1, -1, -1))\n",
    "\n",
    "    file_info.sort(key=lambda x: x[1], reverse=True)  # Sort based on file size in descending order\n",
    "\n",
    "    for info in file_info:\n",
    "        print(\"Size:\", info[1] / 10**6, \"mb\", end=\"\\t\")\n",
    "        print(\"Cols:\", info[2], end=\"\\t\")\n",
    "        print(\"Rows:\", info[3], end=\"\\t\")\n",
    "        print(\"File:\", info[0].replace(fpath, \"\"))\n",
    "\n",
    "# Provide the path to the folder containing the feather files\n",
    "count_feather_files(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07873428d25d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
