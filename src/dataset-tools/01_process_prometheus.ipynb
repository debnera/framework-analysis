{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# import json\n",
    "import ujson as json\n",
    "import os\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "dataset_root = \"../../data/raw_datasets/short-test-2/prometheus.zip\"\n",
    "output_path = \"../../data/processed/short-test-2/\"\n",
    "\n",
    "# Each name specified here will result in a separate .csv file\n",
    "# In addition to workers, our cluster produces data attributed to 'master' and some random IPs\n",
    "worker_names = [\"worker1\", \"worker2\", \"worker3\", \"worker4\", \"worker5\"]\n",
    "\n",
    "        \n"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 1: Read zip\n",
    "\n",
    "Read all files in zip to a dictionary. Each metric results in a separate dictionary entry.\n",
    "\n",
    "These DataFrames then have to be (later on) combined together, while accounting for overlapping columns.\n",
    "\"\"\"\n",
    "\n",
    "# Open as zip\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "names_container = {}\n",
    "values_container = {}\n",
    "timestamps_container = {}\n",
    "with zipfile.ZipFile(dataset_root, 'r') as zip_ref:\n",
    "\n",
    "    # Get a list of all files inside the zip\n",
    "    items = zip_ref.namelist()\n",
    "\n",
    "    # Filter a list of all json files inside the zip\n",
    "    json_files = [x for x in items if x.endswith('.json')]\n",
    "    \n",
    "    i = len(json_files) # Change this to a small number to only ready a limited amount of data (for quick debugging)\n",
    "    # i = 100\n",
    "    count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate over all json files\n",
    "    for path in json_files:\n",
    "        count += 1\n",
    "        print(f\"Progress {count}/{len(json_files):6}, ({count/len(json_files)*100:5.3} %) (time_spent: {time.time() - start_time:.3} s  - avg: {(time.time() - start_time) / count} s)\")\n",
    "        with zip_ref.open(path) as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "\n",
    "            # Iterate over each submetric\n",
    "            for item in json_data['data']['result']:\n",
    "                \n",
    "                # Get name of the metric, metric values and metric timestamps\n",
    "                header = item['metric']\n",
    "                header_str = json.dumps(header)\n",
    "                name = header[\"__name__\"]\n",
    "                values = dict(item['values'])\n",
    "                stamps, vals = list(zip(*item['values']))\n",
    "\n",
    "                # Add new dict key if it does not exist\n",
    "                if header_str not in values_container:\n",
    "                    values_container[header_str] = []\n",
    "                    timestamps_container[header_str] = []\n",
    "\n",
    "                # Add metric to dict\n",
    "                values_container[header_str].extend(values)\n",
    "                timestamps_container[header_str].extend(stamps)\n",
    "                \n",
    "                # Track sub_headers for each file (TODO: elaborate?)\n",
    "                if name not in names_container:\n",
    "                    names_container[name] = []\n",
    "                names_container[name].append(header_str)\n",
    "        i -= 1\n",
    "        if i <= 0:\n",
    "            print(\"Manually specified limit reached. Stopping here.\")\n",
    "        "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f11bcb333a67998",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 2: Sort and filter metrics\n",
    "\n",
    "Sort the metrics inside the dictionary \n",
    "\n",
    "- Filter out redundant features (e.g., if the \"name\" column is \"worker1\" for all rows of a metric, then that column is redundant)\n",
    "- Sort metrics by worker (e.g., create a new dict for each worker, where each dict contains data only from that worker)  \n",
    "\"\"\"\n",
    "\n",
    "def get_unique_keys(headers) -> list:\n",
    "    \"\"\" Get all keys that have differing values between each feature \"\"\"\n",
    "    # print(headers)\n",
    "    filtered_data: list[dict] = [json.loads(x) for x in headers]\n",
    "\n",
    "    # Verify that all features have the same keys\n",
    "    key_sets = [set(x.keys()) for x in filtered_data]\n",
    "    combined_set = set()\n",
    "    for key_set in key_sets:\n",
    "        combined_set.update(key_set)\n",
    "    if len(key_sets[0]) != len(combined_set):\n",
    "        print(\"WARNING: Features have different keys!\")\n",
    "        return []\n",
    "\n",
    "    # Get all keys that have differing values between each feature\n",
    "    different_keys = []\n",
    "    for key in combined_set:\n",
    "        # print(key)\n",
    "        values = [data[key] for data in filtered_data if key in data]\n",
    "        if len(set(values)) > 1:\n",
    "            different_keys.append(key)\n",
    "    return different_keys\n",
    "\n",
    "def divide_to_workers(headers: list):\n",
    "    headers_by_worker = {x: [] for x in worker_names}\n",
    "    for header in headers:\n",
    "        # print(header)\n",
    "        json_data = json.loads(header)\n",
    "        if \"instance\" not in json_data:\n",
    "            continue\n",
    "        instance = json_data[\"instance\"]\n",
    "        if instance not in worker_names:\n",
    "            continue\n",
    "        headers_by_worker[instance].append(header)\n",
    "    return headers_by_worker\n",
    "\n",
    "def get_unique_column_name(header, unique_keys):\n",
    "    header_dict = json.loads(header)\n",
    "    differing_keys = unique_keys\n",
    "    postfix = \"\"\n",
    "    for key in differing_keys:\n",
    "        postfix += f\"_{key}_{header_dict[key]}\"\n",
    "    return header_dict[\"__name__\"] + postfix\n",
    "\n",
    "def get_unique_column_name_2(header, unique_keys):\n",
    "    \"\"\" Old style, without key \"\"\"\n",
    "    header_dict = json.loads(header)\n",
    "    differing_keys = unique_keys\n",
    "    postfix = \"\"\n",
    "    for key in differing_keys:\n",
    "        postfix += f\"_{header_dict[key]}\"\n",
    "    return header_dict[\"__name__\"] + postfix\n",
    "\n",
    "df_by_worker = {}\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "import numpy as np\n",
    "\n",
    "for name, headers in names_container.items():\n",
    "    count += 1\n",
    "    print(f\"Progress {count}/{len(names_container)}, ({count/len(names_container)*100} %) (time_spent: {time.time() - start_time} s)\")\n",
    "    headers_by_worker = divide_to_workers(headers)\n",
    "    # print(headers_by_worker)\n",
    "    prev = set()\n",
    "    for worker, worker_headers in headers_by_worker.items():\n",
    "        if len(worker_headers) == 0:\n",
    "            continue\n",
    "        unique_keys = get_unique_keys(worker_headers)\n",
    "        if len(unique_keys) > 1:\n",
    "            continue  # TODO: This discards a lot of data\n",
    "        \n",
    "        for x in worker_headers:\n",
    "            # unique_name = get_unique_column_name(x, unique_keys)\n",
    "            unique_name = get_unique_column_name_2(x, unique_keys)\n",
    "            # print(worker + \"_\" + unique_name)\n",
    "            sub_df = pd.DataFrame({unique_name: values_container[x]}, index=timestamps_container[x], dtype=np.float32)\n",
    "            # new = set(sub_df.index)\n",
    "            new = set(timestamps_container[x])\n",
    "            if prev != new:\n",
    "                print(f\"{min(new)} {max(new)}\")\n",
    "            prev = new\n",
    "            # print(sub_df)\n",
    "            if worker not in df_by_worker:\n",
    "                df_by_worker[worker] = []\n",
    "            # df_by_worker[worker].update(sub_df)\n",
    "            df_by_worker[worker].append(sub_df)\n",
    "    # for header in headers:\n",
    "    #     print(header)\n",
    "    #     json_data = json.loads(header)\n",
    "    #     instance = json_data[\"instance\"]\n",
    "    #     if instance not in [\"worker1\", \"worker2\", \"worker3\", \"worker4\", \"worker5\"]:\n",
    "    #         continue\n",
    "    #     print(json_data[\"__name__\"])\n",
    "    #     print(json_data[\"instance\"])\n",
    "    # break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa4711055d2cf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Step 3: Combine data and save as separate DataFrames\n",
    "\n",
    "Combine metrics for each worker into a DataFrame. Each DataFrame contains data relevant to only one worker.\n",
    "\n",
    "- Creates multiple DataFrames\n",
    "- Saves those DataFrames as feather-files (that is supported by pandas)\n",
    "- Try to filter out duplicated rows, if they exist\n",
    "\"\"\"\n",
    "\n",
    "batch_len = 1500\n",
    "dfs = {}\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "for worker in worker_names:\n",
    "    df = pd.DataFrame()\n",
    "    if worker not in df_by_worker:\n",
    "        print(f\"Error: {worker} not found in df_by_worker\")\n",
    "        continue\n",
    "    subdfs = df_by_worker[worker]\n",
    "    subdfs_nodup_index = [x[~x.index.duplicated(keep='first')] for x in subdfs]\n",
    "    for i in range(1, len(subdfs_nodup_index), batch_len):\n",
    "        print(f\"{worker}: {i}: {i/len(subdfs_nodup_index)}\")\n",
    "        end = min(i+batch_len, len(subdfs_nodup_index))\n",
    "        x = pd.concat(subdfs_nodup_index[i:end], axis=1)\n",
    "        x = x.loc[:,~x.columns.duplicated()]\n",
    "        # x = x.dropna(axis=0)\n",
    "        df = pd.concat([df, x], axis=1)\n",
    "    df = df.loc[:,~df.columns.duplicated()]  # TODO: Where are these duplicate columns coming from?\n",
    "    dfs[worker] = df\n",
    "    df.to_feather(os.path.join(output_path, worker + \".feather\"))\n",
    "    print(f\"Saved {worker}.feather\")\n",
    "    # w1.drop_duplicates()\n",
    "    # w1[~w1.columns.duplicated(keep='first')]\n",
    "    # w1[~w1.index.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "## Too many cols, col names and values are duplicated\n",
    "# len(set(w1.columns.to_list()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb97bd62acb6e387",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "Stuff below includes some unsorted testing and testing -- not directly related to processing the datasets\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "---------------------------------------------------------------------------------------------------------\n",
    "\"\"\""
   ],
   "id": "41bb6cb4459a516e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "w1 = dfs[worker_names[0]]\n",
    "print(len(w1.columns))\n",
    "print(len(set(w1.columns)))\n",
    "print(len(w1.index))\n",
    "print(len(set(w1.index)))\n",
    "w1.describe()\n",
    "# w1.dropna(axis=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15a6fe7a373e044a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(w1.loc[:,~w1.columns.duplicated()].columns))\n",
    "print(len(set(w1.loc[:,~w1.columns.duplicated()].columns)))\n",
    "print(len(w1.loc[:,~w1.columns.duplicated()].index))\n",
    "print(len(set(w1.loc[:,~w1.columns.duplicated()].index)))\n",
    "w1.loc[:,~w1.columns.duplicated()].dropna(axis=0)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "664f1704f0fdc16d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "w1_subdfs = df_by_worker[\"worker1\"]\n",
    "print(len(w1_subdfs))\n",
    "# for i in range(100):\n",
    "w1 = pd.concat(w1_subdfs[0:100], axis=0)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d55eace1ca6bf635",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "w1 = pd.concat(w1_subdfs[0:100], axis=0)\n",
    "w1\n",
    "\n",
    "## Too many rows, all other cols are always nan"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e1b407a6af78b77",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "w1 = pd.concat(w1_subdfs[0:100], axis=1)\n",
    "w1\n",
    "\n",
    "## Too many cols, col names and values are duplicated"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5bca502455095d67",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "w1 = w1_subdfs[0]\n",
    "for i in range(1, 100):\n",
    "    w1 = w1.join(w1_subdfs[i], how='left', lsuffix='_left', rsuffix='_right')\n",
    "w1\n",
    "# Crashes because it causes duplicate columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2de8892424778d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "w1 = w1_subdfs[0]\n",
    "for i in range(1, 100):\n",
    "    w1 = w1.merge(w1_subdfs[i])\n",
    "w1\n",
    "## MergeError: No common columns to perform merge on."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d2123cd529fb75b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "w1 = w1_subdfs[0]\n",
    "for i in range(1, 100):\n",
    "    w1.update(w1_subdfs[i])\n",
    "w1\n",
    "## MergeError: No common columns to perform merge on."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "657f28be7239eb3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "list(df_by_worker.values())[0]\n",
    "splits = 100\n",
    "w1 = []\n",
    "for split in range(splits):\n",
    "    w1.append(pd.concat(df_by_worker[\"worker1\"][split:-1:splits]))\n",
    "    print(split)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acfce778fbcb7161",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df_by_worker[\"worker1\"][0].describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec5bee1d8353bfd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "no_dup = [x[~x.index.duplicated(keep='first')] for x in df_by_worker[\"worker1\"]]\n",
    "b = pd.concat(no_dup, axis=1)\n",
    "print(b.index)\n",
    "print(len(set(b.index)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f0ca3b4cc3f2a22",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "len(b.columns) * len(b.index)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5ba3f48cb8d9e8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "nan_counts = b.isna().sum().sum()\n",
    "print(nan_counts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9be4705ce60ab91",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "b = b.loc[:, ~b.columns.duplicated()].copy()\n",
    "static_columns = [column for column in b.columns if b[column].nunique() == 1]\n",
    "#pprint(f\"The following columns have static values: {static_columns}\")\n",
    "b.drop(columns=static_columns, inplace=True)\n",
    "\n",
    "# Find all monotonically increasing columns.\n",
    "# - These values might be accumulative and needs additional processing to be useful. (delta over time)\n",
    "# - Some of these values as simple timers (thus useless)\n",
    "# - For example, joules are shown as a cumulative sum since the start of the experiment.\n",
    "#  -- To get the energy consumption at given time, we need to compute the delta between consecutive rows\n",
    "monotonic_columns = [column for column in b.columns if b[column].is_monotonic_increasing]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "603793159960a5c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "monotonic_columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ec68aeb522540b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "b[column]\n",
    "c_df = b.copy()\n",
    "c_df.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "189db7642415ba1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "c_df = c_df.loc[:, ~c_df.columns.duplicated()].copy()\n",
    "c_df[column].describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fff02fe9073c6db6",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "no_dup[1]\n",
    "print(no_dup[1].index)\n",
    "print(len(set(no_dup[1].index)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee3f8e6b38b96f94",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "b.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "375c26b7d4b399ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(df_by_worker[\"worker1\"])):\n",
    "    d = df_by_worker[\"worker1\"][i]\n",
    "    # print(f\"{i}: {len(d.index)} == {len(set(d.index))}\")\n",
    "    c = Counter(d.index)\n",
    "    print(c)\n",
    "    break\n",
    "df_by_worker[\"worker1\"][i]\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22a4ce353c7dce79",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "a = df_by_worker[\"worker1\"][0].join(df_by_worker[\"worker1\"][1])\n",
    "print(a.index)\n",
    "print(len(set(a.index)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aa153aab269a8b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "a = df_by_worker[\"worker1\"][0].merge(df_by_worker[\"worker1\"][1])\n",
    "a.index"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2aed568bbcb2ec9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataframes\n",
    "df1 = pd.DataFrame({'A': [10, 11], 'B': [20, 21]}, index=[1, 2])\n",
    "df2 = pd.DataFrame({ 'C': [30, 31]}, index=[1, 3])\n",
    "\n",
    "# Concatenate along columns\n",
    "combined_df = pd.concat([df1, df2], axis=1)\n",
    "print(combined_df)\n",
    "\n",
    "# combined_df = pd.merge(df1, df2)\n",
    "# print(combined_df)\n",
    "\n",
    "combined_df = df1.join(df2)\n",
    "print(combined_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fac12eed4424d80",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "w1 = []\n",
    "a = pd.concat(df_by_worker[\"worker1\"][0:1000])\n",
    "# ww1 = pd.concat(w1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71a1a4bc958ca104",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(set(a.index)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9edb23bb93532e41",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(a.index))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d24ac71e0276274",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "len(df_by_worker[\"worker1\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97bf06a351c7c3f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "for name, df in df_by_worker.items():\n",
    "    # df.to_csv(os.path.join(output_path, name + \".csv\"), index=False)\n",
    "    df.to_feather(os.path.join(output_path, name + \".feather\"))\n",
    "    print(f\"Saved {name}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff7573f200eaf1d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "prev = set()\n",
    "full = set()\n",
    "for key, val in timestamps_container.items():\n",
    "    new = set(val)\n",
    "    if prev != new:\n",
    "        print(f\"{min(new)}-{max(new)} ({len(new)})\")\n",
    "    prev = new\n",
    "    full = full.union(new)\n",
    "print(\"fin\")\n",
    "print(f\"{min(full)}-{max(full)} ({len(full)})\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4b23686f99992c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "batch_len = 100\n",
    "prev = set()\n",
    "w1 = pd.DataFrame()\n",
    "w1_subdfs = df_by_worker[\"worker1\"]\n",
    "w1_subdfs_nodup = [x[~x.index.duplicated(keep='first')] for x in w1_subdfs]\n",
    "for i in range(1, len(w1_subdfs_nodup), batch_len):\n",
    "    print(f\"{i}: {i/len(w1_subdfs_nodup)}\")\n",
    "    end = min(i+batch_len, len(w1_subdfs_nodup))\n",
    "    x = pd.concat(w1_subdfs_nodup[i:end], axis=1)\n",
    "    x = x.loc[:,~x.columns.duplicated()]\n",
    "    new = set(x.index)\n",
    "    if prev != new:\n",
    "        print(f\"{min(new)}-{max(new)}\")\n",
    "    prev = new\n",
    "    # x = x.dropna(axis=0)\n",
    "    w1 = pd.concat([w1,x], axis=1)\n",
    "w1\n",
    "# w1.drop_duplicates()\n",
    "# w1[~w1.columns.duplicated(keep='first')]\n",
    "# w1[~w1.index.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "## Too many cols, col names and values are duplicated\n",
    "# len(set(w1.columns.to_list()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96672685b3b16dd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "prev = set()\n",
    "w1 = pd.DataFrame()\n",
    "w1_subdfs = df_by_worker[\"worker1\"]\n",
    "w1_subdfs_nodup = [x[~x.index.duplicated(keep='first')] for x in w1_subdfs]\n",
    "for sub in w1_subdfs:\n",
    "    # print(f\"{i}: {i/len(w1_subdfs_nodup)}\")\n",
    "\n",
    "    new = set(sub.index)\n",
    "    if prev != new:\n",
    "        print(f\"{min(new)}-{max(new)}\")\n",
    "    prev = new\n",
    "    # x = x.dropna(axis=0)\n",
    "\n",
    "# w1.drop_duplicates()\n",
    "# w1[~w1.columns.duplicated(keep='first')]\n",
    "# w1[~w1.index.duplicated(keep='first')]\n",
    "\n",
    "\n",
    "## Too many cols, col names and values are duplicated\n",
    "# len(set(w1.columns.to_list()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a37e6943e0feb620",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4220b3cf15416a14",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
